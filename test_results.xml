<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="1" skipped="0" tests="18" time="47.054" timestamp="2024-05-04T12:42:40.461861" hostname="ad12a3ca-02.cloud.together.ai"><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0016-ToyModel]" time="5.546" /><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0016-ToyModelWithTiedWeights]" time="4.402" /><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0001-ToyModel]" time="4.264" /><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0001-ToyModelWithTiedWeights]" time="5.236" /><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.01-ToyModel]" time="4.118" /><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.01-ToyModelWithTiedWeights]" time="4.207" /><testcase classname="tests.test_ddp_individual_parameters" name="test_DistributedDataParallelIndividualParameters[ToyModel]" time="3.934" /><testcase classname="tests.test_ddp_individual_parameters" name="test_DistributedDataParallelIndividualParameters[ToyModelWithTiedWeights]" time="4.948" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_forward_pass_pytorch" time="0.158" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_forward_pass_triton" time="0.844" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_x_pytorch" time="0.255" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_g_pytorch" time="0.001" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_x_triton" time="0.051" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_g_triton" time="0.002" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_autograd_pytorch_forward_backward" time="0.001" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_autograd_triton_forward_backward" time="0.002" /><testcase classname="tests.test_sharded_optimizer" name="test_sharded_optimizer[ToyModel]" time="4.256" /><testcase classname="tests.test_sharded_optimizer" name="test_sharded_optimizer[ToyModelWithTiedWeights]" time="2.373"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 0 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/home/c-xiongb/spring2024-assignment2-systems/cs336-systems/tests/test_sharded_optimizer.py&quot;, line 34, in _test_sharded_optimizer&#10;    device = _setup_process_group(rank=rank, world_size=world_size, backend=&quot;gloo&quot;)&#10;  File &quot;/home/c-xiongb/spring2024-assignment2-systems/cs336-systems/tests/common.py&quot;, line 90, in _setup_process_group&#10;    dist.init_process_group(backend, rank=rank, world_size=world_size)&#10;  File &quot;/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py&quot;, line 86, in wrapper&#10;    func_return = func(*args, **kwargs)&#10;  File &quot;/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py&quot;, line 1177, in init_process_group&#10;    store, rank, world_size = next(rendezvous_iterator)&#10;  File &quot;/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/rendezvous.py&quot;, line 246, in _env_rendezvous_handler&#10;    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)&#10;  File &quot;/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/rendezvous.py&quot;, line 174, in _create_c10d_store&#10;    return TCPStore(&#10;torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:12355 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:12355 (errno: 98 - Address already in use).">model_class = &lt;class 'tests.common.ToyModelWithTiedWeights'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    def test_sharded_optimizer(model_class):
        world_size = 2
&gt;       mp.spawn(
            _test_sharded_optimizer,
            args=(world_size, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_sharded_optimizer.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x14fc6dea9a20&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/home/c-xiongb/spring2024-assignment2-systems/cs336-systems/tests/test_sharded_optimizer.py", line 34, in _test_sharded_optimizer
E           device = _setup_process_group(rank=rank, world_size=world_size, backend="gloo")
E         File "/home/c-xiongb/spring2024-assignment2-systems/cs336-systems/tests/common.py", line 90, in _setup_process_group
E           dist.init_process_group(backend, rank=rank, world_size=world_size)
E         File "/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 86, in wrapper
E           func_return = func(*args, **kwargs)
E         File "/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1177, in init_process_group
E           store, rank, world_size = next(rendezvous_iterator)
E         File "/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
E           store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
E         File "/home/c-xiongb/spring2024-assignment2-systems/336_a2_test_venv/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 174, in _create_c10d_store
E           return TCPStore(
E       torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:12355 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:12355 (errno: 98 - Address already in use).

336_a2_test_venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase></testsuite></testsuites>